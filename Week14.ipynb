{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a69547-5595-4db3-ba0e-d00cc9c39f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/17 22:41:03 WARN Instrumentation: [ee13875d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.regression import LinearRegression \n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "# Initialize Spark Session \n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data \n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns) \n",
    "\n",
    "# Prepare data for modeling \n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df) \n",
    "\n",
    "# Train a linear regression model \n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target') \n",
    "model = lr.fit(df_transformed) \n",
    "\n",
    "# Print model coefficients \n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad055b90-c975-4789-9d28-8aa59fd0edef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057930705247,4.087352266992818]\n",
      "Intercept: 11.568912728188634\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Dataset\n",
    "data = [(1, [2.0, 3.0], 0), (2, [1.0, 5.0], 1), (3, [2.5, 4.5], 1), (4, [3.0, 6.0], 0)]\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "to_vector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df = df.withColumn(\"Features\", to_vector(\"Features\"))\n",
    "\n",
    "# Train Logistic Regression\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Output\n",
    "print(\"Coefficients:\", model.coefficients)\n",
    "print(\"Intercept:\", model.intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0efc82ae-18a8-49cf-a66a-40adc98057cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([5.33333333, 5.33333333]), array([15., 15.])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Practice: KMeans Clustering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Example dataset\n",
    "data = [(1, [1.0, 1.0]), (2, [5.0, 5.0]), (3, [10.0, 10.0]), (4, [15.0, 15.0])]\n",
    "columns = ['ID', 'Features']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "to_vector = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df = df.withColumn(\"Features\", to_vector(\"Features\"))\n",
    "\n",
    "# Train KMeans clustering model\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "# Show cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(f'Cluster Centers: {centers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c7c96d9-f42b-48c2-a3c8-d2a75eed7736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/hadoop/.cache/kagglehub/datasets/uciml/iris/versions/2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Iris.csv', 'database.sqlite']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Homework \n",
    "import kagglehub \n",
    "import os \n",
    "\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"uciml/iris\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "784c462a-cde1-49ab-9b07-9b8613e74d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 03:10:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: double (nullable = true)\n",
      " |-- SepalWidthCm: double (nullable = true)\n",
      " |-- PetalLengthCm: double (nullable = true)\n",
      " |-- PetalWidthCm: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset \n",
    "spark = SparkSession.builder.appName(\"IrisClassification\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\n",
    "    path + \"/Iris.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34ef278c-e88d-4be2-ac3f-caa44b4f27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"SepalLengthCm\",\n",
    "        \"SepalWidthCm\",\n",
    "        \"PetalLengthCm\",\n",
    "        \"PetalWidthCm\"\n",
    "    ],\n",
    "    outputCol=\"Features\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbf6c903-a642-4d32-8645-350a07178fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         Features|Label|\n",
      "+-----------------+-----+\n",
      "|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "|[4.9,3.0,1.4,0.2]|  0.0|\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|\n",
      "|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "|[5.0,3.6,1.4,0.2]|  0.0|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encoding String menjadi Numerik\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"Species\",\n",
    "    outputCol=\"Label\"\n",
    ")\n",
    "\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "df.select(\"Features\", \"Label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733b7c07-1e40-4ad0-9236-20c787496e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"Features\",\n",
    "    labelCol=\"Label\"\n",
    ")\n",
    "\n",
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "796af3a8-3d05-47cb-ad7f-cd1a5cdbca6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi Model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93f5df2a-f223-4e8d-84ae-b596e09c2a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Coefficient Matrix:\n",
      "DenseMatrix([[-1.17604933,  2.36600666, -0.98323465, -2.11817066],\n",
      "             [ 0.63471523, -0.69736921, -0.15942961, -0.95409922],\n",
      "             [ 0.5413341 , -1.66863745,  1.14266425,  3.07226988]])\n",
      "Best Intercept Vector:\n",
      "[5.666092129745435,2.0110613568025393,-7.677153486547974]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross Validation\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "# Train model\n",
    "cvModel = cv.fit(train)\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "print(\"Best Coefficient Matrix:\")\n",
    "print(bestModel.coefficientMatrix)\n",
    "\n",
    "print(\"Best Intercept Vector:\")\n",
    "print(bestModel.interceptVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70470ec-437b-498c-a619-d073272c30e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
